# Bitcoin Education Roleplay Environment Configuration

# =============================================================================
# OLLAMA CONFIGURATION
# =============================================================================

# Ollama API URL - Configure for your deployment
# Local development: Use VITE proxy (leave empty or set to /api)
# Server deployment: Set to your Ollama server URL
# Examples:
# - Local: (empty) or /api
# - Same server: http://localhost:11434
# - Remote server: http://your-server.com:11434
# - Docker: http://ollama-container:11434
VITE_OLLAMA_URL=

# Ollama model to use
# Recommended models:
# - llama3.2:3b (fast, good for 4-core systems)
# - phi3:mini (faster, good for 8+ core systems)
# - llama3.2:1b (fastest, limited responses)
VITE_OLLAMA_MODEL=llama3.2:3b

# Ollama request timeout (milliseconds)
# Increase for slower systems or longer responses
VITE_OLLAMA_TIMEOUT=120000

# =============================================================================
# BACKEND CONFIGURATION
# =============================================================================

# Backend API URL - Configure for your deployment
# Local development: Use VITE proxy (leave empty or set to /knowledge)
# Server deployment: Set to your backend server URL
# Examples:
# - Local: (empty) or /knowledge
# - Same server: http://localhost:3000
# - Remote server: http://your-server.com:3000
# - Docker: http://backend-container:3000
VITE_BACKEND_URL=

# =============================================================================
# CHAT CONFIGURATION
# =============================================================================

# Maximum number of messages to keep in chat history
VITE_CHAT_MAX_HISTORY=20

# Health check interval (milliseconds)
VITE_HEALTH_CHECK_INTERVAL=30000

# =============================================================================
# UI CONFIGURATION
# =============================================================================

# Mobile breakpoint (pixels)
VITE_MOBILE_BREAKPOINT=768

# Maximum message length
VITE_MAX_MESSAGE_LENGTH=1000

# =============================================================================
# AVATAR CONFIGURATION
# =============================================================================

# Default avatar model
VITE_AVATAR_MODEL=kei

# Enable avatar fallback (true/false)
VITE_AVATAR_FALLBACK=true

# =============================================================================
# DEPLOYMENT EXAMPLES
# =============================================================================

# LOCAL DEVELOPMENT (default - no environment file needed)
# All services run on localhost, Vite proxies handle routing

# SAME SERVER DEPLOYMENT
# VITE_OLLAMA_URL=http://localhost:11434
# VITE_BACKEND_URL=http://localhost:3000

# REMOTE OLLAMA SERVER
# VITE_OLLAMA_URL=http://your-ollama-server.com:11434
# VITE_BACKEND_URL=http://localhost:3000

# DOCKER DEPLOYMENT
# VITE_OLLAMA_URL=http://ollama:11434
# VITE_BACKEND_URL=http://backend:3000

# CLOUD DEPLOYMENT
# VITE_OLLAMA_URL=https://your-ollama-api.com
# VITE_BACKEND_URL=https://your-backend-api.com 